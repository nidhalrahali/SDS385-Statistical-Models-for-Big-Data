---
title: "gradient decent"
author: "Yinan Zhu"
date: "September 27, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r}
library(readr)
data <- read_csv("~/GitHub/SDS385-course-work/Excercise 1/gradient decent/wdbc.csv",col_names = FALSE)
source('~/GitHub/SDS385-course-work/Excercise 1/gradient decent/gradient decent functions.R')
X=as.matrix(data[3:12])
X=scale(X)
X=cbind(X,1)
y=as.vector(matrix(nrow=nrow(data),ncol=1))
for(i in 1:nrow(data)){
  if(data[i,2]=="M")y[i]=1
  else y[i]=0
}
beta0=as.vector(matrix(0,nrow=11))

trainX=X[1:250,]
trainy=y[1:250]
testX=X[251:569,]
testy=y[251:569]
```


```{r}
ite=200
eps=0.05
result=gradientdecent(trainX,trainy,beta0,eps,ite)
test_negloglikelihood = rep(0,ite)
train_negloglikelihood = rep(0,ite)
for(i in 1 : ite){
og = omega(testX,result[,i])
test_negloglikelihood[i] = nllh(og,testy)/length(testy)
og = omega(trainX, result[,i])
train_negloglikelihood[i] = nllh(og,trainy)/length(trainy)
}
plot(train_negloglikelihood,type='l',ylab='negative loglikelihood',xlab='',sub='gradient search eps=0.05')
lines(test_negloglikelihood,col='red')
fix_nllh=train_negloglikelihood
```

```{r}
ite=200
eps0=1
alpha=0.9
c=0.1
result=gradientdecent_linesearch(trainX,trainy,beta0,eps0,ite,alpha,c)
test_negloglikelihood = rep(0,ite)
train_negloglikelihood = rep(0,ite)
for(i in 1 : ite){
og = omega(testX,result$betahistory[,i])
test_negloglikelihood[i] = nllh(og,testy)/length(testy)
og = omega(trainX, result$betahistory[,i])
train_negloglikelihood[i] = nllh(og,trainy)/length(trainy)
}
plot(train_negloglikelihood,type='l',ylab='negative loglikelihood',xlab='',sub='gradient search backtracking c=0.1')
lines(test_negloglikelihood,col='red')
bigc_nllh=train_negloglikelihood
```
```{r}
plot(result$epshistory,type='l',xlab='',ylab='step size')
```
```{r}
c=0.001
result=gradientdecent_linesearch(trainX,trainy,beta0,eps0,ite,alpha,c)
test_negloglikelihood = rep(0,ite)
train_negloglikelihood = rep(0,ite)
for(i in 1 : ite){
og = omega(testX,result$betahistory[,i])
test_negloglikelihood[i] = nllh(og,testy)/length(testy)
og = omega(trainX, result$betahistory[,i])
train_negloglikelihood[i] = nllh(og,trainy)/length(trainy)
}
plot(train_negloglikelihood,type='l',ylab='negative loglikelihood',xlab='',sub='gradient search backtracking c=0.001')
lines(test_negloglikelihood,col='red')
smallc_nllh=train_negloglikelihood
```
```{r}
plot(result$epshistory,type='l',xlab='',ylab='step size')
```
```{r}
plot(fix_nllh,type='l',xlab='',ylab='negative loglikelihood')
lines(bigc_nllh,col='blue')
lines(smallc_nllh,col='purple')
```
```{r}
c=0.01
ite=200
result=quasi_newtonmethod(trainX,trainy,beta0,eps0,ite,alpha,c)
test_negloglikelihood = rep(0,ite)
train_negloglikelihood = rep(0,ite)
for(i in 1 : ite){
og = omega(testX,result$betahistory[,i])
test_negloglikelihood[i] = nllh(og,testy)/length(testy)
og = omega(trainX, result$betahistory[,i])
train_negloglikelihood[i] = nllh(og,trainy)/length(trainy)
}
plot(train_negloglikelihood,type='l',ylab='negative loglikelihood',xlab='',sub='quasi newton method')
lines(test_negloglikelihood,col='red')
qnm_nllh=train_negloglikelihood
```

```{r}
ite=20
result=newtonmethod(trainX,trainy,beta0,ite)
test_negloglikelihood = rep(0,ite)
train_negloglikelihood = rep(0,ite)
for(i in 1 : ite){
og = omega(testX,result[,i])
test_negloglikelihood[i] = nllh(og,testy)/length(testy)
og = omega(trainX, result[,i])
train_negloglikelihood[i] = nllh(og,trainy)/length(trainy)
}
plot(train_negloglikelihood,type='l',ylab='negative loglikelihood',xlab='',sub='newton method')
lines(test_negloglikelihood,col='red')
nm_nllh=train_negloglikelihood
```

```{r}
plot(fix_nllh,type='l',xlab='',ylab='negative loglikelihood')
lines(bigc_nllh,col='blue')
lines(smallc_nllh,col='purple')
lines(qnm_nllh,col='green')
lines(nm_nllh,col='yellow')
```
