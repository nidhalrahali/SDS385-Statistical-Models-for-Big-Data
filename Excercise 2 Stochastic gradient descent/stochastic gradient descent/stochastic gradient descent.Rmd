---
title: "Stochastic gradient descent"
author: "Yinan Zhu"
date: "September 27, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r}
library(readr)
data <- read_csv("~/GitHub/SDS385-course-work/Excercise 2 Stochastic gradient descent/stochastic gradient descent/wdbc.csv",col_names = FALSE)
source('~/GitHub/SDS385-course-work/Excercise 2 Stochastic gradient descent/stochastic gradient descent/gradient descent functions.R')
```
```{r}
X=as.matrix(data[3:12])
X=scale(X)
X=cbind(X,1)
y=as.vector(matrix(nrow=nrow(data),ncol=1))
for(i in 1:nrow(data)){
  if(data[i,2]=="M")y[i]=1
  else y[i]=0
}
beta0=as.vector(matrix(0,nrow=11))

trainX=X[1:400,]
trainy=y[1:400]
testX=X[401:569,]
testy=y[401:569]
```


```{r}
ite=1000
alpha=0.05
```
we use moving average with exponential decay to compute "averagenegloglikelihood": averageX(t)=X(t)*alpha+averageX(t-1)*(1-alpha)

black line is the moving average of loglikelihood computed using exponential decay:averageX(t)=X(t)*alpha+averageX(t-1)*(1-alpha), red is the negative loglikelihood per data point on the test data
```{r}
eps=0.001
result=stochasticgradientdescent(trainX,trainy,testX,testy,beta0,eps,ite,alpha)
plot(result$averagenegloglikelihood,type='l',xlab='',ylab='negative loglikelihood',sub='step size=0.001')
lines(result$testnegloglikelihoood,col='red')
legend('topright',legend=c('training data (moving average)','test data'),lty=1,col=c('black','red'))
```

```{r}
eps=0.005
result=stochasticgradientdescent(trainX,trainy,testX,testy,beta0,eps,ite,alpha)
plot(result$averagenegloglikelihood,type='l',xlab='iteratiion',ylab='negative loglikelihood',sub='eps=0.005')
lines(result$testnegloglikelihoood,col='red')
legend('topright',legend=c('training data (moving average)','test data'),lty=1,col=c('black','red'))
```
```{r}
eps=0.02
result=stochasticgradientdescent(trainX,trainy,testX,testy,beta0,eps,ite,alpha)
plot(result$averagenegloglikelihood,type='l',xlab='',ylab='negative loglikelihood',sub='eps=0.02')
lines(result$testnegloglikelihoood,col='red')
legend('topright',legend=c('training data (moving average)','test data'),lty=1,col=c('black','red'))
```


varying steps, we use the Robbins-Monro rule where (step size at t)=C/(t+t0)^decay
```{r}
t0=1
C=0.5
```

```{r}
decay=0.95
result=varyingstepsgradientdescent(trainX,trainy,testX,testy,beta0,ite,alpha,decay,t0,C)
plot(result$averagenegloglikelihood,type='l',ylab='negative loglikelihood',xlab='iteration',sub='decay=0.95')
lines(result$testnegloglikelihoood,col='red')
legend('topright',legend=c('training data (moving average)','test data'),lty=1,col=c('black','red'))
```

```{r}
decay=0.75
result=varyingstepsgradientdescent(trainX,trainy,testX,testy,beta0,ite,alpha,decay,t0,C)
plot(result$averagenegloglikelihood,type='l',ylab='negative loglikelihood',xlab='iteration',sub='decay=0.75')
lines(result$testnegloglikelihoood,col='red')
legend('topright',legend=c('training data (moving average)','test data'),lty=1,col=c('black','red'))
```

```{r}
decay=0.6
result=varyingstepsgradientdescent(trainX,trainy,testX,testy,beta0,ite,alpha,decay,t0,C)
plot(result$averagenegloglikelihood,type='l',ylab='negative loglikelihood',xlab='iteration',sub='decay=0.6')
lines(result$testnegloglikelihoood,col='red')
legend('topright',legend=c('training data (moving average)','test data'),lty=1,col=c('black','red'))
```
