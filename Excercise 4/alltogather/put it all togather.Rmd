---
title: "Stochastic gradient decent"
author: "Yinan Zhu"
date: "September 27, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r}
library(readr)
library(Matrix)
source('~/GitHub/SDS385-course-work/Excercise 4/alltogather/gradient decent functions.R')
```

```{r}
# Where are the files stored?
base_dir = "~/url_svmlight/"
svm_files = dir(base_dir, pattern = "*.svm")

# Loop through the files and create a list of objects
X_list = list()
y_list = list()
for(i in seq_along(svm_files)) {
	myfile = svm_files[i]
	cat(paste0("Reading file ", i, ": ", myfile, "\n"))
	D = read_svmlight_class(paste0(base_dir, myfile), num_cols = 3231962)
	X_list[[i]] = D$features
	y_list[[i]] = D$labels
}
```
```{r}
X=new("dgCMatrix")
for(i in 1:121){
  print(i)
  X=rBind(X,X_list[[i]])
}
```
```{r}
#Assemble one matrix of features/vector of responses (do.call very handy here, although not super efficient
X = do.call(rBind, X_list)  # rBind, not rbind, for sparse matrices
y = do.call(c, y_list)
y = 0 + {y==1}

# Save as serialized (binary) files for much faster read-in next time
saveRDS(X, file='url_X.rds')
saveRDS(y, file='url_y.rds')
```

```{r}
X=as.matrix(data[3:12])
X=scale(X)
X=cbind(X,1)
y=as.vector(matrix(nrow=nrow(data),ncol=1))
for(i in 1:nrow(data)){
  if(data[i,2]=="M")y[i]=1
  else y[i]=0
}
beta0=as.vector(matrix(0,nrow=11))

trainX=X[1:250,]
trainy=y[1:250]
testX=X[251:569,]
testy=y[251:569]
```


```{r}
ite=1000
```

```{r}
eps=1
result=sgd_adagrad(trainX,trainy,beta0,eps,ite)
test_negloglikelihood = rep(0,ite)
train_negloglikelihood = rep(0,ite)
for(i in 1 : ite){
og = omega(testX,result[,i])
test_negloglikelihood[i] = nllh(og,testy)/length(testy)
og = omega(trainX, result[,i])
train_negloglikelihood[i] = nllh(og,trainy)/length(trainy)
}
plot(train_negloglikelihood,type='l',ylab='negative loglikelihood',xlab='',sub='gradient search with ada_grad')
lines(test_negloglikelihood,col='red')
adagrad_nllh=train_negloglikelihood
```



